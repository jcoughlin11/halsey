1.) Zero the RNN state properly and where appropriate
    * https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb
    zeros the rnn state twice: once at the beginning of each episode, and then
    again right before doing the double dqn update.
    * At the beginning of each episode, it is initialized to a tuple of two np
    arrays, each [1,h_size], where h_size is the number of output classes, I
    think (the number of units in the last FC layer). Before doing a double dqn
    update, the state is reinitialized to a tuple of np arrays of shape
    [batch_size, h_size].
    * He uses two different variables for the initialized states. state is for
    the start of episode reinitialization, and train_state is before doing the
    double dqn update.
    * The rnn state is updated after choosing an action, and that's it. This
    update is set to state1.
    * At the end of the step, state=state1 and the loop starts over until the
    episode ends.
    * This means, as far as I can tell, when doing the double dqn, he's always
    using a zeroed rnn state, as state_train never changes.

5.) Maybe make trainParams a dictionary? Then I don't need all of the gross
    unpacking I currently have.

6.) The save/load due to early stopping needs to be refined. Mainly, what
    happens if a parameter (such as the length of the memory buffer) changes?
    That screws everything up. Basically, all of the parameters from the
    parameter file need to be saved, as well, ensuring that everything remains
    the same across runs.

8.) When using a custom loss function, the argument(s) it requires have to be
    passed to it at model compilation time (according to the article I read,
    see the notes in losses.py). My question is: are those arguments passed
    by reference or by value? That is, if I pass, say, a = 10 as an arg at
    compilation time but I want a to change as training continues, does keras
    know about those changes made to a, or is its value forever stuck at 10 in
    Keras' eyes? This is important because, as far as I can tell, there is no
    way to pass external variables to the loss function during training
    (high-level APIs are both a blessing and a curse...)

    I might need to write a custom layer to do this sort of thing:
    https://github.com/keras-team/keras/issues/1061

    From my experimenting, it appears as if the custom loss function uses pass
    by value, which is frustrating.

    I'm also not convinced that the custom loss function used for importance
    sampling is the same as just passing sample_weight=isWeights. (see Baselines
    by openAI, specifically the function build_train in deepq/build_graph.py)

    Looking at the tf source code for train_on_batch when sample_weight is set,
    it ends up calling a function called scale_losses_by_sample_weight
    (/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/util.py).
    This function literally just multiplies the losses by the weights. I think
    this is the same as what OpenAI does in baselines because they get:
    tderror = q_target - q
    error = huber_loss(tderror)
    weightederror = tf.reduce_mean(isWeights * error)

    So they are scaling the losses, not the td errors.

9.) Get tdError in each learn function and return it so that the PER memory can
    be updated at the end of learn().

10.) Put in a time-limit option so the code auto-stops and saves if it's not
    done by that point (mainly for use on clusters).

11.) The sample_weight argument supports temporal data, which I kind of have in
    the form of the stack of frames. It might be interesting to weight each frame
    in the stack differently based on how far it is from the current frame.

12.) When saving, check to make sure there's enough HDD space
