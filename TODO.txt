1.) Zero the RNN state properly and where appropriate
    * https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb
    zeros the rnn state twice: once at the beginning of each episode, and then
    again right before doing the double dqn update.
    * At the beginning of each episode, it is initialized to a tuple of two np
    arrays, each [1,h_size], where h_size is the number of output classes, I
    think (the number of units in the last FC layer). Before doing a double dqn
    update, the state is reinitialized to a tuple of np arrays of shape
    [batch_size, h_size].
    * He uses two different variables for the initialized states. state is for
    the start of episode reinitialization, and train_state is before doing the
    double dqn update.
    * The rnn state is updated after choosing an action, and that's it. This
    update is set to state1.
    * At the end of the step, state=state1 and the loop starts over until the
    episode ends.
    * This means, as far as I can tell, when doing the double dqn, he's always
    using a zeroed rnn state, as state_train never changes.
