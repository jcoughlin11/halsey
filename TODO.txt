1.) Zero the RNN state properly and where appropriate
    * https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb
    zeros the rnn state twice: once at the beginning of each episode, and then
    again right before doing the double dqn update.
    * At the beginning of each episode, it is initialized to a tuple of two np
    arrays, each [1,h_size], where h_size is the number of output classes, I
    think (the number of units in the last FC layer). Before doing a double dqn
    update, the state is reinitialized to a tuple of np arrays of shape
    [batch_size, h_size].
    * He uses two different variables for the initialized states. state is for
    the start of episode reinitialization, and train_state is before doing the
    double dqn update.
    * The rnn state is updated after choosing an action, and that's it. This
    update is set to state1.
    * At the end of the step, state=state1 and the loop starts over until the
    episode ends.
    * This means, as far as I can tell, when doing the double dqn, he's always
    using a zeroed rnn state, as state_train never changes.

5.) Maybe make trainParams a dictionary? Then I don't need all of the gross
    unpacking I currently have.

6.) The save/load due to early stopping needs to be refined. Mainly, what
    happens if a parameter (such as the length of the memory buffer) changes?
    That screws everything up. Basically, all of the parameters from the
    parameter file need to be saved, as well, ensuring that everything remains
    the same across runs.

7.) In losses.py, check isWeights (both type and multiplication). I don't think
    it can be a np array (I think it has to be a tensor). Also, I'm pretty sure
    I want element-wise multiplication, not a dot product-type operation.
    Just using a * b (if a and b are tensors) is element-wise multiplication.
    The two tensors need to have the same shape, so isWeights needs to have the
    same shape as y_true and y_pred (make sure that it does).

8.) When using a custom loss function, the argument(s) it requires have to be
    passed to it at model compilation time (according to the article I read,
    see the notes in losses.py). My question is: are those arguments passed
    by reference or by value? That is, if I pass, say, a = 10 as an arg at
    compilation time but I want a to change as training continues, does keras
    know about those changes made to a, or is its value forever stuck at 10 in
    Keras' eyes? This is important because, as far as I can tell, there is no
    way to pass external variables to the loss function during training
    (high-level APIs are both a blessing and a curse...)

    I might need to write a custom layer to do this sort of thing:
    https://github.com/keras-team/keras/issues/1061

    Look at the PER paper... See what loss they used

    APPARENTLY: keras.fit has a keyword arg called sample_weight that does
    exactly this. Just pass the isWeights as sample_weight.
