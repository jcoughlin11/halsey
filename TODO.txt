1.) Zero the RNN state properly and where appropriate
    * https://github.com/awjuliani/DeepRL-Agents/blob/master/Deep-Recurrent-Q-Network.ipynb
    zeros the rnn state twice: once at the beginning of each episode, and then
    again right before doing the double dqn update.
    * At the beginning of each episode, it is initialized to a tuple of two np
    arrays, each [1,h_size], where h_size is the number of output classes, I
    think (the number of units in the last FC layer). Before doing a double dqn
    update, the state is reinitialized to a tuple of np arrays of shape
    [batch_size, h_size].
    * He uses two different variables for the initialized states. state is for
    the start of episode reinitialization, and train_state is before doing the
    double dqn update.
    * The rnn state is updated after choosing an action, and that's it. This
    update is set to state1.
    * At the end of the step, state=state1 and the loop starts over until the
    episode ends.
    * This means, as far as I can tell, when doing the double dqn, he's always
    using a zeroed rnn state, as state_train never changes.

2.) Need to update the loss = sess.run call at the end of learn() to use
    train_on_batch.

3.) Need to get rid of all other sess.run calls.

4.) Using model.predict works on batches, and so a batch_size parameter needs
    to be specified. If it isn't, keras defaults to the value of 32. Do I need
    to set it differently?

5.) Maybe make trainParams a dictionary? Then I don't need all of the gross
    unpacking I currently have.

6.) The save/load due to early stopping needs to be refined. Mainly, what
    happens if a parameter (such as the length of the memory buffer) changes?
    That screws everything up. Basically, all of the parameters from the
    parameter file need to be saved, as well, ensuring that everything remains
    the same across runs.

7.) Vectorize the qTarget loop in learn() by using a mask

8.) Check the array shapes in learn()

9.) Compare my training loop with that of keon.io. This is a shape thing.
    Basically, my qTarget is an array with shape = (batchSize,), so it's just
    one number for each batch. His has shape = (batchSize, nActions), so you
    get the full Q-value vector for each sample in the batch. Think about
    when I did the digit classification. The labels were all one-hot vectors
    representing the correct class, i.e., label = [1,0,0,...] for a 0. That is,
    the full output vector was given, so I think that's what I need here.
